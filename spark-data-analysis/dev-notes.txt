-this project is for developing data analysis procedures in scala to run on spark.  the basic workflow is like this:
  -generic data transformations can be written in here (things that don't use spark libraries)
  -analysis procedures that call spark libraries can be written in here
  -we may choose to annotate these different things so that a program can provide a ui to use    these different procedures however a user sees fit
  -in order to run on spark, we build everything into an uber jar (excluding spark libraries)    using 'mvn clean install', and submit the jar to a spark cluster, passing the arguments        spark needs to run the procedure (main class, jar location, args, etc)
  -we will probably build a ui tool to make it easy to submit jobs.  we may want to modularize    our analysis procedures so that we can easily chain operations without having to create new    files to do so.  i.e. from the ui, we could have buttons for different analysis procedures,    and create a chain of operations by just clicking the buttons we want in the order we want.

-file setup for executing this stuff on spark...
  i believe we can write all our analysis procedures to take all the spark stuff as parameters (like the SparkContext object and/or the dataset we are working with), and then specify the analysis procedure as an arg to our main method, and have some service that fetches the procedure and then calls it with the SparkContext that the main method creates.  then we should be able to (with some modifications probably) chain the analysis procedures relatively easily.

-data format and usage...
  many of the analysis procedures expect specific data structures.  for example, many of the machine learning procedures, like classification expect an RDD of LabeledPoint objects - so we want the file format to be rows of [classifier vector_of_values].  so we need to transform our raw data into another format to be used by the procedure (either in memory or store it on the file system).  so...we need some things:
  -need more procedures for transforming data?
  -at the very least, we can't just use price data how i was thinking.
   i think what we might want to do is have some service that loads the price data into
   a specific format.  so what we really want may be classes that just map the price data to
   certain formats.

  clarifying: if an SVM procedure expects a LabeledPoint, then it expects all the features to
  already be computed for the data point.  so we need a process of computing all these
  features, and knowing what to load (if we store the computed stuff in files).  for now i
  would prefer to compute on the fly (to save file space until we know what we're doing for
  sure).  so it seems like the first step here is to load price data with spark, and then
  map it to an appropriate representation based on what we pass in to it.
